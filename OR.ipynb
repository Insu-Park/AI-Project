'''

인공지능 

Multi-Layer perceptron 구현

서울시립대학교 컴퓨터과학부 2014920017 박인수

AND, OR, XOR 비교 그래프 구현

OR GATE 구현

'''

import tensorflow as tf
import numpy as np
from numpy import *
from numpy.random import multivariate_normal
import matplotlib.pyplot as plt
import pandas as pd
from pandas import Series, DataFrame

# Hidden Layer 없이 단층으로 구현
# Optimizer = Adam

x = tf.placeholder(tf.float32, [None, 2])
w = tf.Variable(tf.zeros([2,1]))
w0 = tf.Variable(tf.zeros([1]))
y = tf.sigmoid(tf.matmul(x,w) + w0)
t = tf.placeholder(tf.float32, [None, 1])
loss = tf.reduce_sum(tf.square(y-t))
train_step = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 50개의 난수 데이터로 학습, 얼마나 정확하게 학습하는지 확인하기 위해 시드는 주지않음
train_x = np.zeros([50,2])
train_t = np.zeros([50,1])
for row in range(0, 50):
  for col in range(0,2):
    train_x[row][col] = np.random.randint(2)
    # 난수 데이터에 따른 target 지정
    if((train_x[row][0] == 0 and train_x[row][1] == 0)):
      train_t[row][0] = 0
    else:
      train_t[row][0] = 1


# Iteration Graph 세팅
fig1 = plt.figure(figsize=(8,8))
iter_subplot = fig1.add_subplot(1, 1, 1)
loss_valarr = []

# Learning Graph 세팅
fig2 = plt.figure(figsize=(8,8))
learning_subplot = fig2.add_subplot(1, 1, 1)
learning_subplot.set_xlim(-0.2, 1.2)
learning_subplot.set_ylim(-0.2, 1.2)
linex = np.linspace(-0.2,1.2, 400)

i = 0
for _ in range(10000):
  i+= 1
  sess.run(train_step, feed_dict={x:train_x, t:train_t})
  loss_val = sess.run(loss, feed_dict={x:train_x, t:train_t})
  # 실행할 때마다 loss를 배열로 저장
  loss_valarr.append(loss_val)
  if i% 2000 == 0:    
    print('Step: %d, Loss: %f' % (i, loss_val))
    w0_val, w_val = sess.run([w0, w])
    w0_val, w1_val, w2_val = w0_val[0], w_val[0][0], w_val[1][0]
    print(w0_val, w1_val, w2_val)    
    liney = -(w1_val*linex/w2_val + w0_val/w2_val)
    # 2000번 실행할 때마다 그래프에 표시
    learning_subplot.plot(linex, liney, linewidth=2, label='%d' %(i))
    learning_subplot.legend(loc='upper left')

# loss 배열을  그래프로 표시
iter_subplot.plot(range(len(loss_valarr)), loss_valarr,linewidth=2, label='Error')
iter_subplot.legend(loc='upper left')    

# X1, X2 좌표에 대해 데이터를 점으로 표시 - (0,0), (0,1), (1,0), (1,1)
train_set = hstack((train_x, train_t))
df = DataFrame(train_set)
df.columns = ["x1", "x2", "t"]
train_set0 = df[df['t']==0]
train_set1 = df[df['t']==1] 
learning_subplot.scatter(train_set1.x1, train_set1.x2, marker='o')
learning_subplot.scatter(train_set0.x1, train_set0.x2, marker='x')
